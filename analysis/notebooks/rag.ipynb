{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a79a946-5727-431a-8abd-ef12e0351800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tobias Fechner\\Documents\\2_Work\\prisma\\evaluate\\analysis\\notebooks\n",
      "neo4j+s://7a2be29f.databases.neo4j.io\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "env_path = Path('..') / '.env.local'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Retrieve credentials\n",
    "uri = os.getenv(\"NEO4J_URI\")\n",
    "username = os.getenv(\"NEO4J_USER\")\n",
    "password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "print(uri)\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7f837-eb2d-4903-af6f-f517beb5a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_vector_index(driver, index_name):\n",
    "    with driver.session() as session:\n",
    "        # Use backticks in case the index name has hyphens\n",
    "        cypher = f\"DROP INDEX `{index_name}` IF EXISTS\"\n",
    "        session.run(cypher)\n",
    "        print(f\"ðŸ—‘ï¸ Dropped vector index: {index_name}\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "drop_vector_index(driver, \"voice-vector-index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cfb35c4-8ea9-431f-aef3-d4cabd639c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"voice-vector-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b6e05-3c7b-46c0-b36b-6f5c441f6c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index parameters\n",
    "NODE_LABEL = \"VoiceChunk\"\n",
    "PROPERTY_NAME = \"embedding\"\n",
    "DIMENSIONS = 3072  # For OpenAI \"text-embedding-3-large\"\n",
    "SIMILARITY_FUNCTION = \"cosine\"  # or 'euclidean' or 'dot'\n",
    "\n",
    "# Create the index\n",
    "def create_vector_index(driver):\n",
    "    cypher = f\"\"\"\n",
    "    CALL db.index.vector.createNodeIndex(\n",
    "        '{INDEX_NAME}',\n",
    "        '{NODE_LABEL}',\n",
    "        '{PROPERTY_NAME}',\n",
    "        {DIMENSIONS},\n",
    "        '{SIMILARITY_FUNCTION}'\n",
    "    )\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        try:\n",
    "            session.run(cypher)\n",
    "            print(f\"âœ… Vector index '{INDEX_NAME}' created successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to create vector index: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7258d5-cb02-4a62-bd0b-7158148ec616",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vector_index(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20f3b1ac-2563-47a5-b206-577ada883a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from neo4j_graphrag.retrievers import VectorRetriever\n",
    "from neo4j_graphrag.llm import OpenAILLM\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "from neo4j_graphrag.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43667405-f91a-4791-818a-02dd50b268bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = VectorRetriever(driver, INDEX_NAME, embedder)\n",
    "\n",
    "# 3. LLM\n",
    "# Note: the OPENAI_API_KEY must be in the env vars\n",
    "llm = OpenAILLM(model_name=\"gpt-4o\", model_params={\"temperature\": 0})\n",
    "\n",
    "# Initialize the RAG pipeline\n",
    "rag = GraphRAG(retriever=retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67837315-2236-4a99-b3e3-1552e4a04f6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************dM8A. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Query the graph\u001b[39;00m\n\u001b[32m      2\u001b[39m query_text = \u001b[33m\"\u001b[39m\u001b[33mList as many tangible stakeholders that were referred to. Exclude Prisma, ARC, Wada and Dream Village. Examples of stakeholders are: companies, communities, institutions, governments etc.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m response = \u001b[43mrag\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretriever_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_k\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.answer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\analysis-51f7fKDf-py3.12\\Lib\\site-packages\\neo4j_graphrag\\generation\\graphrag.py:135\u001b[39m, in \u001b[36mGraphRAG.search\u001b[39m\u001b[34m(self, query_text, message_history, examples, retriever_config, return_context)\u001b[39m\n\u001b[32m    133\u001b[39m     message_history = message_history.messages\n\u001b[32m    134\u001b[39m query = \u001b[38;5;28mself\u001b[39m._build_query(validated_data.query_text, message_history)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m retriever_result: RetrieverResult = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvalidated_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretriever_config\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m context = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(item.content \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m retriever_result.items)\n\u001b[32m    139\u001b[39m prompt = \u001b[38;5;28mself\u001b[39m.prompt_template.format(\n\u001b[32m    140\u001b[39m     query_text=query_text, context=context, examples=validated_data.examples\n\u001b[32m    141\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\analysis-51f7fKDf-py3.12\\Lib\\site-packages\\neo4j_graphrag\\retrievers\\base.py:136\u001b[39m, in \u001b[36mRetriever.search\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> RetrieverResult:\n\u001b[32m    132\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search method. Call the `get_search_results` method that returns\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    a list of `neo4j.Record`, and format them using the function returned by\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[33;03m    `get_result_formatter` to return `RetrieverResult`.\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     raw_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_search_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m     formatter = \u001b[38;5;28mself\u001b[39m.get_result_formatter()\n\u001b[32m    138\u001b[39m     search_items = [formatter(record) \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m raw_result.records]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\analysis-51f7fKDf-py3.12\\Lib\\site-packages\\neo4j_graphrag\\retrievers\\vector.py:197\u001b[39m, in \u001b[36mVectorRetriever.get_search_results\u001b[39m\u001b[34m(self, query_vector, query_text, top_k, effective_search_ratio, filters)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embedder:\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EmbeddingRequiredError(\n\u001b[32m    195\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEmbedding method required for text query.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    196\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m query_vector = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedder\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m parameters[\u001b[33m\"\u001b[39m\u001b[33mquery_vector\u001b[39m\u001b[33m\"\u001b[39m] = query_vector\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m parameters[\u001b[33m\"\u001b[39m\u001b[33mquery_text\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\analysis-51f7fKDf-py3.12\\Lib\\site-packages\\neo4j_graphrag\\embeddings\\openai.py:62\u001b[39m, in \u001b[36mBaseOpenAIEmbeddings.embed_query\u001b[39m\u001b[34m(self, text, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, **kwargs: Any) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m     55\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03m    Generate embeddings for a given query using an OpenAI text embedding model.\u001b[39;00m\n\u001b[32m     57\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m \u001b[33;03m        **kwargs (Any): Additional arguments to pass to the OpenAI embedding generation function.\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     embedding: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m] = response.data[\u001b[32m0\u001b[39m].embedding\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m embedding\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\analysis-51f7fKDf-py3.12\\Lib\\site-packages\\openai\\resources\\embeddings.py:129\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    123\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    124\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    125\u001b[39m             ).tolist()\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\analysis-51f7fKDf-py3.12\\Lib\\site-packages\\openai\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\analysis-51f7fKDf-py3.12\\Lib\\site-packages\\openai\\_base_client.py:1037\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1034\u001b[39m             err.response.read()\n\u001b[32m   1036\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************dM8A. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "# Query the graph\n",
    "query_text = \"List as many tangible stakeholders that were referred to. Exclude Prisma, ARC, Wada and Dream Village. Examples of stakeholders are: companies, communities, institutions, governments etc.\"\n",
    "response = rag.search(query_text=query_text, retriever_config={\"top_k\": 25})\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3605735-f59c-44cc-9ec3-e29d320d6475",
   "metadata": {},
   "source": [
    "### Filter by participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e16cc-e9be-48b9-bdd4-0a2eabd812da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.retrievers import VectorCypherRetriever\n",
    "\n",
    "# Cypher to retrieve only facilitator voice reflections\n",
    "retrieval_query = \"\"\"\n",
    "MATCH (node)<-[:HAS_CHUNK]-(v:Voice)<-[:HAS_VOICE]-(e:Entry)-[:SENT_BY]->(p:Participant)\n",
    "WHERE p.role = 'participant'\n",
    "RETURN node.chunk_text AS content, score\n",
    "\"\"\"\n",
    "\n",
    "# Set up the retriever\n",
    "retriever = VectorCypherRetriever(\n",
    "    driver=driver,\n",
    "    index_name=\"voice-vector-index\",       # Make sure this matches your vector index name\n",
    "    retrieval_query=retrieval_query,\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "# Re-initialize the RAG pipeline\n",
    "rag = GraphRAG(retriever=retriever, llm=llm)\n",
    "\n",
    "# Example query\n",
    "response = rag.search(query_text=\"What were the main sessions that are mentioned by participants? Use only the context provided. Give examples of the reflections for each, using the exact text from the data. Present in the format Session Name: Reflection. Try to retrieve 5 distinct sessions.\", retriever_config={\"top_k\": 15})\n",
    "\n",
    "# Print results\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb8be8f-d4f2-4196-baca-c8dc5a85bced",
   "metadata": {},
   "source": [
    "### Filter by participant AND include chunk parent node for whole context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e407a-7952-4139-8abc-2f9732120699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.types import RetrieverResultItem\n",
    "\n",
    "def result_formatter(record):\n",
    "    chunk = record.get(\"chunk_text\")\n",
    "    full_text = record.get(\"full_transcription\")\n",
    "    score = record.get(\"score\")\n",
    "\n",
    "    # Compose content with chunk + hint of full text\n",
    "    content = f\"Chunk: {chunk}\\n\\nContext: {full_text[:1500]}...\"  # truncate to avoid tokens overflow\n",
    "\n",
    "    return RetrieverResultItem(\n",
    "        content=content,\n",
    "        metadata={\"score\": score}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a0bf68-1c67-4285-945d-dd072ccef26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cypher to retrieve only facilitator voice reflections\n",
    "retrieval_query = \"\"\"\n",
    "MATCH (node:VoiceChunk)<-[:HAS_CHUNK]-(v:Voice)\n",
    "MATCH (v)<-[:HAS_VOICE]-(e:Entry)-[:SENT_BY]->(p:Participant)\n",
    "WHERE p.role = 'participant'\n",
    "RETURN\n",
    "  node.chunk_text AS chunk_text,\n",
    "  v.transcription AS full_transcription,\n",
    "  score\n",
    "\"\"\"\n",
    "\n",
    "# Set up the retriever\n",
    "retriever = VectorCypherRetriever(\n",
    "    driver=driver,\n",
    "    index_name=\"voice-vector-index\",       # Make sure this matches your vector index name\n",
    "    retrieval_query=retrieval_query,\n",
    "    embedder=embedder,\n",
    "    result_formatter=result_formatter\n",
    ")\n",
    "\n",
    "# Re-initialize the RAG pipeline\n",
    "rag = GraphRAG(retriever=retriever, llm=llm)\n",
    "\n",
    "# Example query\n",
    "response = rag.search(query_text=\"What were the main facilitated exercises (workshops, scheduled agenda items, activities, sessions) that the participants found interesting or valuable? Value can be understood in terms of having created a shift in their perspective. Reflective data is favoured. Use only the context provided. Give examples of the reflections for each, using the exact text from the data. Present in the format Session Name: Reflection. Try to retrieve 5 distinct sessions.\", retriever_config={\"top_k\": 25})\n",
    "\n",
    "# Print results\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856be7f8-c660-4c56-85be-c2d5dea51781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd4b03-ed64-413c-8c4f-bff56127737d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
