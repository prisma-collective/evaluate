{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a79a946-5727-431a-8abd-ef12e0351800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "env_path = Path('..') / '.env.local'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Retrieve credentials\n",
    "uri = os.getenv(\"NEO4J_URI\")\n",
    "username = os.getenv(\"NEO4J_USER\")\n",
    "password = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "print(uri)\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7f837-eb2d-4903-af6f-f517beb5a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_vector_index(driver, index_name):\n",
    "    with driver.session() as session:\n",
    "        # Use backticks in case the index name has hyphens\n",
    "        cypher = f\"DROP INDEX `{index_name}` IF EXISTS\"\n",
    "        session.run(cypher)\n",
    "        print(f\"üóëÔ∏è Dropped vector index: {index_name}\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "drop_vector_index(driver, \"voice-vector-index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b6e05-3c7b-46c0-b36b-6f5c441f6c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index parameters\n",
    "INDEX_NAME = \"voice-vector-index\"\n",
    "NODE_LABEL = \"VoiceChunk\"\n",
    "PROPERTY_NAME = \"embedding\"\n",
    "DIMENSIONS = 3072  # For OpenAI \"text-embedding-3-large\"\n",
    "SIMILARITY_FUNCTION = \"cosine\"  # or 'euclidean' or 'dot'\n",
    "\n",
    "# Create the index\n",
    "def create_vector_index(driver):\n",
    "    cypher = f\"\"\"\n",
    "    CALL db.index.vector.createNodeIndex(\n",
    "        '{INDEX_NAME}',\n",
    "        '{NODE_LABEL}',\n",
    "        '{PROPERTY_NAME}',\n",
    "        {DIMENSIONS},\n",
    "        '{SIMILARITY_FUNCTION}'\n",
    "    )\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        try:\n",
    "            session.run(cypher)\n",
    "            print(f\"‚úÖ Vector index '{INDEX_NAME}' created successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to create vector index: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7258d5-cb02-4a62-bd0b-7158148ec616",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vector_index(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3b1ac-2563-47a5-b206-577ada883a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from neo4j_graphrag.retrievers import VectorRetriever\n",
    "from neo4j_graphrag.llm import OpenAILLM\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "from neo4j_graphrag.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43667405-f91a-4791-818a-02dd50b268bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = VectorRetriever(driver, INDEX_NAME, embedder)\n",
    "\n",
    "# 3. LLM\n",
    "# Note: the OPENAI_API_KEY must be in the env vars\n",
    "llm = OpenAILLM(model_name=\"gpt-4o\", model_params={\"temperature\": 0})\n",
    "\n",
    "# Initialize the RAG pipeline\n",
    "rag = GraphRAG(retriever=retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67837315-2236-4a99-b3e3-1552e4a04f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the graph\n",
    "query_text = \"What were the main sessions that are mentioned in the data? Use only the context provided. Give examples of the reflections for each (make it short but keep the exact text).\"\n",
    "response = rag.search(query_text=query_text, retriever_config={\"top_k\": 5})\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1635ba22-8665-455d-8982-0d9b3eed4be2",
   "metadata": {},
   "source": [
    "Below currently doesn't work as we need to chunk large transcripts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3605735-f59c-44cc-9ec3-e29d320d6475",
   "metadata": {},
   "source": [
    "### Filter by participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e16cc-e9be-48b9-bdd4-0a2eabd812da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.retrievers import VectorCypherRetriever\n",
    "\n",
    "# Cypher to retrieve only facilitator voice reflections\n",
    "retrieval_query = \"\"\"\n",
    "MATCH (node)<-[:HAS_CHUNK]-(v:Voice)<-[:HAS_VOICE]-(e:Entry)-[:SENT_BY]->(p:Participant)\n",
    "WHERE p.role = 'participant'\n",
    "RETURN node.chunk_text AS content, score\n",
    "\"\"\"\n",
    "\n",
    "# Set up the retriever\n",
    "retriever = VectorCypherRetriever(\n",
    "    driver=driver,\n",
    "    index_name=\"voice-vector-index\",       # Make sure this matches your vector index name\n",
    "    retrieval_query=retrieval_query,\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "# Re-initialize the RAG pipeline\n",
    "rag = GraphRAG(retriever=retriever, llm=llm)\n",
    "\n",
    "# Example query\n",
    "response = rag.search(query_text=\"What were the main sessions that are mentioned by participants? Use only the context provided. Give examples of the reflections for each, using the exact text from the data. Present in the format Session Name: Reflection. Try to retrieve 5 distinct sessions.\", retriever_config={\"top_k\": 15})\n",
    "\n",
    "# Print results\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb8be8f-d4f2-4196-baca-c8dc5a85bced",
   "metadata": {},
   "source": [
    "### Filter by participant AND include chunk parent node for whole context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e407a-7952-4139-8abc-2f9732120699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j_graphrag.types import RetrieverResultItem\n",
    "\n",
    "def result_formatter(record):\n",
    "    chunk = record.get(\"chunk_text\")\n",
    "    full_text = record.get(\"full_transcription\")\n",
    "    score = record.get(\"score\")\n",
    "\n",
    "    # Compose content with chunk + hint of full text\n",
    "    content = f\"Chunk: {chunk}\\n\\nContext: {full_text[:1500]}...\"  # truncate to avoid tokens overflow\n",
    "\n",
    "    return RetrieverResultItem(\n",
    "        content=content,\n",
    "        metadata={\"score\": score}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a0bf68-1c67-4285-945d-dd072ccef26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cypher to retrieve only facilitator voice reflections\n",
    "retrieval_query = \"\"\"\n",
    "MATCH (node:VoiceChunk)<-[:HAS_CHUNK]-(v:Voice)\n",
    "MATCH (v)<-[:HAS_VOICE]-(e:Entry)-[:SENT_BY]->(p:Participant)\n",
    "WHERE p.role = 'participant'\n",
    "RETURN\n",
    "  node.chunk_text AS chunk_text,\n",
    "  v.transcription AS full_transcription,\n",
    "  score\n",
    "\"\"\"\n",
    "\n",
    "# Set up the retriever\n",
    "retriever = VectorCypherRetriever(\n",
    "    driver=driver,\n",
    "    index_name=\"voice-vector-index\",       # Make sure this matches your vector index name\n",
    "    retrieval_query=retrieval_query,\n",
    "    embedder=embedder,\n",
    "    result_formatter=result_formatter\n",
    ")\n",
    "\n",
    "# Re-initialize the RAG pipeline\n",
    "rag = GraphRAG(retriever=retriever, llm=llm)\n",
    "\n",
    "# Example query\n",
    "response = rag.search(query_text=\"What were the main facilitated exercises (workshops, scheduled agenda items, activities, sessions) that the participants found interesting or valuable? Value can be understood in terms of having created a shift in their perspective. Reflective data is favoured. Use only the context provided. Give examples of the reflections for each, using the exact text from the data. Present in the format Session Name: Reflection. Try to retrieve 5 distinct sessions.\", retriever_config={\"top_k\": 25})\n",
    "\n",
    "# Print results\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856be7f8-c660-4c56-85be-c2d5dea51781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd4b03-ed64-413c-8c4f-bff56127737d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
